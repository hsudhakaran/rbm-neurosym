{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c314e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:6f43c99c-4cde-4ddc-8ed0-466228ef8c6f"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from typing import Callable, Tuple, Any\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d07ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train, Y_train, X_test, Y_test = jnp.expand_dims(jnp.array(X_train/255), axis=-1), jnp.expand_dims(jnp.array(Y_train), axis=-1), jnp.expand_dims(jnp.array(X_test/255), axis=-1), jnp.expand_dims(jnp.array(Y_test), axis=-1)\n",
    "\n",
    "X_train = (X_train > 0.5).astype(jnp.float32)\n",
    "X_test = (X_test > 0.5).astype(jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083af19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. The Energy-Based Model (RBM) ---\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.1):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Initialize weights with small random values\n",
    "        # W shape: (n_visible, n_hidden)\n",
    "        self.W = jax.random.normal(jax.random.PRNGKey(45), (n_visible, n_hidden)) #(0, 0.01, (n_visible, n_hidden))\n",
    "        self.v_bias = jnp.zeros(n_visible)\n",
    "        self.h_bias = jnp.zeros(n_hidden)\n",
    "\n",
    "        # Monitoring\n",
    "        self.errors = []\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + jnp.exp(-x))\n",
    "\n",
    "    def sample_hidden(self, v):\n",
    "        \"\"\"\n",
    "        Block Gibbs Step: P(h|v)\n",
    "        \"\"\"\n",
    "        # Calculate activation energy\n",
    "        print(v.shape, self.W.shape)\n",
    "        activation = jnp.dot(v, self.W) + self.h_bias\n",
    "        prob_h = self.sigmoid(activation)\n",
    "        # Stochastic sampling (Bernoulli)\n",
    "        return prob_h, jax.random.binomial(jax.random.PRNGKey(397), 1, prob_h)\n",
    "\n",
    "    def sample_visible(self, h):\n",
    "        \"\"\"\n",
    "        Block Gibbs Step: P(v|h)\n",
    "        \"\"\"\n",
    "        # Calculate activation energy\n",
    "        activation = jnp.dot(h, self.W.T) + self.v_bias\n",
    "        prob_v = self.sigmoid(activation)\n",
    "        # Stochastic sampling (Bernoulli)\n",
    "        return prob_v, jax.random.binomial(jax.random.PRNGKey(16), 1, prob_v)\n",
    "\n",
    "    def contrastive_divergence(self, v0):\n",
    "        \"\"\"\n",
    "        The training algorithm (CD-1).\n",
    "        Approximates the gradient of the Energy function.\n",
    "        \"\"\"\n",
    "        # --- Positive Phase (Data Driven) ---\n",
    "        # 1. Pass data up to hidden layer\n",
    "        prob_h0, h0 = self.sample_hidden(v0)\n",
    "\n",
    "        # --- Negative Phase (Model Daydreaming / Block Gibbs) ---\n",
    "        # 2. Reconstruct visible (Block Gibbs down)\n",
    "        prob_v1, v1 = self.sample_visible(h0)\n",
    "        # 3. Sample hidden again (Block Gibbs up)\n",
    "        prob_h1, h1 = self.sample_hidden(v1)\n",
    "\n",
    "        # --- Update Weights (Gradient Descent on Energy) ---\n",
    "        # Contrastive Divergence: <v0*h0> - <v1*h1>\n",
    "        batch_size = v0.shape[0]\n",
    "\n",
    "        positive_grad = jnp.dot(v0.T, prob_h0)\n",
    "        negative_grad = jnp.dot(v1.T, prob_h1)\n",
    "\n",
    "        # Update W, biases\n",
    "        self.W += self.lr * (positive_grad - negative_grad) / batch_size\n",
    "        self.v_bias += self.lr * jnp.mean(v0 - v1, axis=0)\n",
    "        self.h_bias += self.lr * jnp.mean(prob_h0 - prob_h1, axis=0)\n",
    "\n",
    "        # Track reconstruction error (MSE) for visualization\n",
    "        error = jnp.mean((v0 - v1) ** 2)\n",
    "        return error\n",
    "\n",
    "    def transform(self, v):\n",
    "        \"\"\"\n",
    "        Transform raw input into hidden feature representation.\n",
    "        Returns the probability of hidden units being active.\n",
    "        \"\"\"\n",
    "        prob_h, _ = self.sample_hidden(v)\n",
    "        return prob_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4bcbda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RBM (Generative Phase)...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (1,) and (784,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-402067377.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrastive_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mepoch_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1760963099.py\u001b[0m in \u001b[0;36mcontrastive_divergence\u001b[0;34m(self, v0)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# --- Positive Phase (Data Driven) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# 1. Pass data up to hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mprob_h0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# --- Negative Phase (Model Daydreaming / Block Gibbs) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1760963099.py\u001b[0m in \u001b[0;36msample_hidden\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Calculate activation energy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprob_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Stochastic sampling (Bernoulli)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/numpy/tensor_contractions.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision, preferred_element_type, out_sharding)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mcontract_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m   result = lax.dot_general(a, b, dimension_numbers=(contract_dims, batch_dims),\n\u001b[0m\u001b[1;32m    123\u001b[0m                            \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                            \u001b[0mpreferred_element_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_element_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[0m\n\u001b[1;32m   5297\u001b[0m     msg = (\"dot_general requires contracting dimensions to have the same \"\n\u001b[1;32m   5298\u001b[0m            \"shape, got {} and {}.\")\n\u001b[0;32m-> 5299\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_contracting_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_contracting_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5301\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_dot_general_shape_computation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (1,) and (784,)."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- 3. Training the PGM (Unsupervised) ---\n",
    "print(\"Training RBM (Generative Phase)...\")\n",
    "n_hidden_units = 128\n",
    "rbm = RBM(n_visible=784, n_hidden=n_hidden_units, learning_rate=0.1)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "n_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_error = 0\n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(X_train.shape[0])\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        batch = X_train[batch_idx]\n",
    "\n",
    "        # Skip incomplete batches\n",
    "        if len(batch) < batch_size: continue\n",
    "\n",
    "        err = rbm.contrastive_divergence(batch)\n",
    "        epoch_error += err\n",
    "\n",
    "    avg_error = epoch_error / n_batches\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Reconstruction Error: {avg_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af43eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 28, 28, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0aa9498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbf2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
